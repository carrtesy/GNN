{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cora with pytorch geometric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root=\"data\", name = \"Cora\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora()\n",
      "1\n",
      "7\n",
      "1433\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(len(dataset))\n",
    "print(dataset.num_classes)\n",
    "print(dataset.num_node_features)\n",
    "print(dataset.num_edge_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.edge_index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ True,  True,  True,  ..., False, False, False])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.train_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2708, 1433])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 4, 4,  ..., 3, 3, 3])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data.x[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "data = dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 32)\n",
    "        self.conv2 = GCNConv(32, 16)\n",
    "        self.conv3 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, data = GCN().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2, weight_decay=5e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.9410877227783203\n",
      "Loss: 1.8530105352401733\n",
      "Loss: 1.749307632446289\n",
      "Loss: 1.5974714756011963\n",
      "Loss: 1.4630048274993896\n",
      "Loss: 1.313302993774414\n",
      "Loss: 1.1401771306991577\n",
      "Loss: 0.9885607957839966\n",
      "Loss: 0.8730276226997375\n",
      "Loss: 0.7507880330085754\n",
      "Loss: 0.6332305073738098\n",
      "Loss: 0.6043334603309631\n",
      "Loss: 0.5287462472915649\n",
      "Loss: 0.41219156980514526\n",
      "Loss: 0.42277616262435913\n",
      "Loss: 0.3233049511909485\n",
      "Loss: 0.30313315987586975\n",
      "Loss: 0.2742003798484802\n",
      "Loss: 0.21048885583877563\n",
      "Loss: 0.1837950348854065\n",
      "Loss: 0.20244139432907104\n",
      "Loss: 0.13810287415981293\n",
      "Loss: 0.11276200413703918\n",
      "Loss: 0.09405592083930969\n",
      "Loss: 0.12875646352767944\n",
      "Loss: 0.15877306461334229\n",
      "Loss: 0.08602245151996613\n",
      "Loss: 0.06638729572296143\n",
      "Loss: 0.18255291879177094\n",
      "Loss: 0.07169251888990402\n",
      "Loss: 0.11679030954837799\n",
      "Loss: 0.0990569218993187\n",
      "Loss: 0.08418361097574234\n",
      "Loss: 0.10605935007333755\n",
      "Loss: 0.08982821553945541\n",
      "Loss: 0.07161252945661545\n",
      "Loss: 0.09290781617164612\n",
      "Loss: 0.0444791242480278\n",
      "Loss: 0.04541029408574104\n",
      "Loss: 0.04514820873737335\n",
      "Loss: 0.06004604697227478\n",
      "Loss: 0.04195950925350189\n",
      "Loss: 0.03640572726726532\n",
      "Loss: 0.04913150891661644\n",
      "Loss: 0.040418751537799835\n",
      "Loss: 0.05728888511657715\n",
      "Loss: 0.06070012226700783\n",
      "Loss: 0.06281690299510956\n",
      "Loss: 0.0313437320291996\n",
      "Loss: 0.029111891984939575\n",
      "Loss: 0.03491302579641342\n",
      "Loss: 0.05247548967599869\n",
      "Loss: 0.056515563279390335\n",
      "Loss: 0.04467226564884186\n",
      "Loss: 0.04642168805003166\n",
      "Loss: 0.019854187965393066\n",
      "Loss: 0.050902727991342545\n",
      "Loss: 0.04015639051795006\n",
      "Loss: 0.054545819759368896\n",
      "Loss: 0.031012190505862236\n",
      "Loss: 0.05505130812525749\n",
      "Loss: 0.059809908270835876\n",
      "Loss: 0.04553724452853203\n",
      "Loss: 0.06718911230564117\n",
      "Loss: 0.029836539179086685\n",
      "Loss: 0.024036159738898277\n",
      "Loss: 0.033682771027088165\n",
      "Loss: 0.0740317851305008\n",
      "Loss: 0.041962359100580215\n",
      "Loss: 0.04706692323088646\n",
      "Loss: 0.03739699721336365\n",
      "Loss: 0.024527613073587418\n",
      "Loss: 0.028475157916545868\n",
      "Loss: 0.05681401118636131\n",
      "Loss: 0.053055696189403534\n",
      "Loss: 0.032167110592126846\n",
      "Loss: 0.06929194927215576\n",
      "Loss: 0.04208295792341232\n",
      "Loss: 0.030632128939032555\n",
      "Loss: 0.03461012616753578\n",
      "Loss: 0.028900712728500366\n",
      "Loss: 0.07423698902130127\n",
      "Loss: 0.05148579552769661\n",
      "Loss: 0.02769693173468113\n",
      "Loss: 0.05358109250664711\n",
      "Loss: 0.046595893800258636\n",
      "Loss: 0.023480774834752083\n",
      "Loss: 0.019156552851200104\n",
      "Loss: 0.025424838066101074\n",
      "Loss: 0.034265074878931046\n",
      "Loss: 0.045575570315122604\n",
      "Loss: 0.04458751901984215\n",
      "Loss: 0.038151707500219345\n",
      "Loss: 0.033544786274433136\n",
      "Loss: 0.03521083667874336\n",
      "Loss: 0.03978554159402847\n",
      "Loss: 0.05564047768712044\n",
      "Loss: 0.05124783515930176\n",
      "Loss: 0.050093185156583786\n",
      "Loss: 0.04013849422335625\n",
      "Loss: 0.042343758046627045\n",
      "Loss: 0.05818861722946167\n",
      "Loss: 0.05961580201983452\n",
      "Loss: 0.04892749339342117\n",
      "Loss: 0.03590083867311478\n",
      "Loss: 0.031944599002599716\n",
      "Loss: 0.04769974946975708\n",
      "Loss: 0.05707132816314697\n",
      "Loss: 0.027559995651245117\n",
      "Loss: 0.054519474506378174\n",
      "Loss: 0.03448069095611572\n",
      "Loss: 0.02289043925702572\n",
      "Loss: 0.03925594314932823\n",
      "Loss: 0.0197784174233675\n",
      "Loss: 0.01730685867369175\n",
      "Loss: 0.039098940789699554\n",
      "Loss: 0.03902871906757355\n",
      "Loss: 0.031355321407318115\n",
      "Loss: 0.02660326100885868\n",
      "Loss: 0.027909284457564354\n",
      "Loss: 0.06267480552196503\n",
      "Loss: 0.03794201835989952\n",
      "Loss: 0.03920162469148636\n",
      "Loss: 0.038565125316381454\n",
      "Loss: 0.04627622291445732\n",
      "Loss: 0.042727671563625336\n",
      "Loss: 0.024271612986922264\n",
      "Loss: 0.06279604136943817\n",
      "Loss: 0.04133491590619087\n",
      "Loss: 0.03022238239645958\n",
      "Loss: 0.030167756602168083\n",
      "Loss: 0.036663781851530075\n",
      "Loss: 0.038395605981349945\n",
      "Loss: 0.038263555616140366\n",
      "Loss: 0.03188948705792427\n",
      "Loss: 0.03019484505057335\n",
      "Loss: 0.030880212783813477\n",
      "Loss: 0.03193676471710205\n",
      "Loss: 0.014764292165637016\n",
      "Loss: 0.015471495687961578\n",
      "Loss: 0.021581187844276428\n",
      "Loss: 0.03234301134943962\n",
      "Loss: 0.025561466813087463\n",
      "Loss: 0.03749436140060425\n",
      "Loss: 0.04403700679540634\n",
      "Loss: 0.06450707465410233\n",
      "Loss: 0.020478686317801476\n",
      "Loss: 0.02889612317085266\n",
      "Loss: 0.01611216738820076\n",
      "Loss: 0.028800733387470245\n",
      "Loss: 0.03949499502778053\n",
      "Loss: 0.027261460199952126\n",
      "Loss: 0.014952942728996277\n",
      "Loss: 0.014498996548354626\n",
      "Loss: 0.025798797607421875\n",
      "Loss: 0.025188958272337914\n",
      "Loss: 0.021050993353128433\n",
      "Loss: 0.023509293794631958\n",
      "Loss: 0.046967729926109314\n",
      "Loss: 0.02095276117324829\n",
      "Loss: 0.03342769294977188\n",
      "Loss: 0.010309931822121143\n",
      "Loss: 0.014551670290529728\n",
      "Loss: 0.03054056130349636\n",
      "Loss: 0.02602636069059372\n",
      "Loss: 0.02551882341504097\n",
      "Loss: 0.02117326855659485\n",
      "Loss: 0.023631716147065163\n",
      "Loss: 0.030225137248635292\n",
      "Loss: 0.019522489979863167\n",
      "Loss: 0.038632046431303024\n",
      "Loss: 0.028195256367325783\n",
      "Loss: 0.03540347144007683\n",
      "Loss: 0.019101524725556374\n",
      "Loss: 0.0638466328382492\n",
      "Loss: 0.03595881909132004\n",
      "Loss: 0.019680224359035492\n",
      "Loss: 0.029201483353972435\n",
      "Loss: 0.026499111205339432\n",
      "Loss: 0.027149982750415802\n",
      "Loss: 0.02526257373392582\n",
      "Loss: 0.02450667880475521\n",
      "Loss: 0.03093584254384041\n",
      "Loss: 0.025124825537204742\n",
      "Loss: 0.016346609219908714\n",
      "Loss: 0.028237292543053627\n",
      "Loss: 0.021231384947896004\n",
      "Loss: 0.033185869455337524\n",
      "Loss: 0.04575160890817642\n",
      "Loss: 0.035930655896663666\n",
      "Loss: 0.02821144089102745\n",
      "Loss: 0.03370680287480354\n",
      "Loss: 0.040925707668066025\n",
      "Loss: 0.026068994775414467\n",
      "Loss: 0.023152535781264305\n",
      "Loss: 0.04227299615740776\n",
      "Loss: 0.036482542753219604\n",
      "Loss: 0.0351574532687664\n",
      "Loss: 0.026233525946736336\n",
      "Loss: 0.03848375752568245\n",
      "Loss: 0.016418013721704483\n",
      "Loss: 0.01840919256210327\n",
      "Loss: 0.022025002166628838\n",
      "Loss: 0.07535889744758606\n",
      "Loss: 0.0301692895591259\n",
      "Loss: 0.01683206856250763\n",
      "Loss: 0.025595931336283684\n",
      "Loss: 0.029134700074791908\n",
      "Loss: 0.030288822948932648\n",
      "Loss: 0.025717804208397865\n",
      "Loss: 0.018854321911931038\n",
      "Loss: 0.043133486062288284\n",
      "Loss: 0.04495282471179962\n",
      "Loss: 0.01406076829880476\n",
      "Loss: 0.02447189949452877\n",
      "Loss: 0.028647052124142647\n",
      "Loss: 0.02711845189332962\n",
      "Loss: 0.03142554312944412\n",
      "Loss: 0.03274685516953468\n",
      "Loss: 0.01034255139529705\n",
      "Loss: 0.03229294717311859\n",
      "Loss: 0.03261172026395798\n",
      "Loss: 0.031086983159184456\n",
      "Loss: 0.014453647658228874\n",
      "Loss: 0.027701860293745995\n",
      "Loss: 0.012856624089181423\n",
      "Loss: 0.020861158147454262\n",
      "Loss: 0.016795504838228226\n",
      "Loss: 0.028435729444026947\n",
      "Loss: 0.02241591177880764\n",
      "Loss: 0.021171653643250465\n",
      "Loss: 0.01400274783372879\n",
      "Loss: 0.04538101330399513\n",
      "Loss: 0.04959028959274292\n",
      "Loss: 0.023391837254166603\n",
      "Loss: 0.01740596443414688\n",
      "Loss: 0.012071629986166954\n",
      "Loss: 0.058948468416929245\n",
      "Loss: 0.04032314196228981\n",
      "Loss: 0.03561100363731384\n",
      "Loss: 0.01757671870291233\n",
      "Loss: 0.027683410793542862\n",
      "Loss: 0.013968225568532944\n",
      "Loss: 0.016940364614129066\n",
      "Loss: 0.060932185500860214\n",
      "Loss: 0.030100375413894653\n",
      "Loss: 0.02746332623064518\n",
      "Loss: 0.09090079367160797\n",
      "Loss: 0.019283998757600784\n",
      "Loss: 0.03700985386967659\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(250):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Loss: {loss.item()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}